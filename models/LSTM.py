# -*- coding: utf-8 -*-
"""embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZXEKGtppVK4o3fJXbiabwd2Gi0uGacJ
"""

import torch.optim as optim
import torch
import matplotlib.pyplot as plt
import numpy as np
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from torchvision import datasets, transforms
import torch
import torch
from torch.autograd import Variable
import torch.nn.functional as F
from torch import nn
!pip install transformers
from transformers import RobertaTokenizer
import numpy as np
import pickle
import pdb
import tqdm
import csv
import re
from sklearn.model_selection import train_test_split
import pandas as pd
import torch
from torch.utils.data import Dataset

import os
import pickle
import csv
import pdb
from collections import Counter

tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')

class FunLineDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        super(FunLineDataset, self).__init__()
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):

        return list(self.data.loc[idx])

def tokenize(sentence, tokenizer):
    tokens = tokenizer(sentence, return_tensors='pt')
    return tokens

def create_dataset(path):
    data = []
    with open(path, encoding="utf8", errors='ignore') as f:
        rows = csv.reader(f)
        rows = list(rows)
        for row in rows[1:]:
            # if float(row[4]) == 0:
            #     continue
            match = re.search(r'<.*>', row[1])
            original = row[1][:match.start()] + row[1][match.start() + 1:match.end()-2] + row[1][match.end():]
            original_ids = tokenize(original, tokenizer_roberta)['input_ids'][0]
            edited = row[1][:match.start()] + row[2] + row[1][match.end():]
            edited_ids = tokenize(edited, tokenizer_roberta)['input_ids'][0]
            data.append((original_ids, edited_ids, float(row[4])))

    train, test = train_test_split(data, test_size=0.2, random_state=1)
    val, test = train_test_split(test, test_size=0.5, random_state=1)

    train = pd.DataFrame(
        train, columns=['Original', 'Ebdited', 'Mean_Score'])
    val = pd.DataFrame(
        val, columns=['Original', 'Edited', 'Mean_Score'])
    test = pd.DataFrame(
        test, columns=['Original', 'Edited', 'Mean_Score'])
    return train, val, test
    
train, val, test = create_dataset('../data/train_lines.csv')



def collate_fn(batch):
    """
    Create a batch of data given a list of N sequences and labels. Sequences are stacked into a single tensor
    of shape (N, max_sequence_length), where max_sequence_length is the maximum length of any sequence in the
    batch. Sequences shorter than this length should be filled up with 0's. Also returns a tensor of shape (N, 1)
    containing the label of each sequence.

    :param batch: A list of size N, where each element is a tuple containing a sequence tensor and a single item
    tensor containing the true label of the sequence.

    :return: A tuple containing two tensors. The first tensor has shape (N, max_sequence_length) and contains all
    sequences. Sequences shorter than max_sequence_length are padded with 0s at the end. The second tensor
    has shape (N, 1) and contains all labels.
    """
    org, edit, labels = zip(*batch)
    padded_org = torch.nn.utils.rnn.pad_sequence(org, batch_first=True,padding_value=1)
    padded_edit = torch.nn.utils.rnn.pad_sequence(edit, batch_first=True,padding_value=1)
    labels = torch.Tensor(labels)
    return (padded_org, padded_edit, labels)

batch_size = 256

trainset = FunLineDataset(train)
trainloader = torch.utils.data.DataLoader(
    trainset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)

valset = FunLineDataset(val)
valloader = torch.utils.data.DataLoader(
    valset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)

testset = FunLineDataset(test)
testloader = torch.utils.data.DataLoader(
    testset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn)

class LSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        embedding_dim = 100
        self.embedding = nn.Embedding(num_embeddings=50257, embedding_dim=embedding_dim, padding_idx=1)


        self.hidden = 512
        self.orig = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden, batch_first=True)
        self.edit = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden, batch_first=True)


        self.fc1 = nn.Linear(2*self.hidden, self.hidden)
        self.fc2 = nn.Linear(self.hidden, self.hidden//2)
        self.fc3 = nn.Linear(self.hidden//2, 1)

        self.d1 = nn.Dropout(0.3)
        self.d2 = nn.Dropout(0.3)
        self.d3 = nn.Dropout(0.3)
        self.d4 = nn.Dropout(0.3)

    
    def forward(self, x, x_edited):
      x = self.embedding(x)  #[32, 28, 64]
      x_edited = self.embedding(x_edited)

      x, h = self.orig(x) #[32, 28, 128]
      x_edited, h = self.edit(x_edited)
      x = F.leaky_relu(self.d1(x))
      x_edited = F.leaky_relu(self.d2(x_edited))

      x = x[:,-1,:]
      x_edited = x_edited[:,-1,:]
      input = torch.cat([x, x_edited], dim=1).reshape(-1, 2*self.hidden)
      input = F.leaky_relu(self.d3(self.fc1(input)))  
      input =  F.leaky_relu(self.d3(self.fc2(input)))
      input = self.fc3(input)
      return input

from tqdm import tqdm
num_epochs = 50
learning_rate = 0.001
net = LSTMModel().to(device)

loss_fn = torch.nn.MSELoss()
opt = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)

def train():

  RMSE_train = []
  RMSE_valid = []
  for epoch in range(num_epochs):
      net.train()
      losses = []
      print("Training : ", epoch+1)
      for i, batch in enumerate(trainloader):
        x, x_edited, y = batch
        y = y.to(device)
        x = x.to(device)
        x_edited = x_edited.to(device)

        y_hat = net(x, x_edited).squeeze()
        # y_hat = y_hat.double()

        loss = loss_fn(y_hat, y)
        losses.append(loss)
        opt.zero_grad()
        loss.backward()
        opt.step()

      print(f"Average RMSE loss for Train: {torch.sqrt(torch.mean(torch.Tensor(losses)))}")
      RMSE_train.append(torch.sqrt(torch.mean(torch.Tensor(losses))))
      RMSE_valid.append(validate())
      print()
  print('done training')
  return RMSE_train, RMSE_valid

def validate():
  net.eval()
  losses = []
  for i, batch in enumerate(valloader):
    x, x_edited, y = batch
    y = y.to(device)
    x = x.to(device)
    x_edited = x_edited.to(device)
    y_hat = net(x, x_edited).squeeze()
    loss = loss_fn(y_hat, y)
    losses.append(loss)
  print(f"Average RMSE loss for validation: {torch.sqrt(torch.mean(torch.Tensor(losses)))}")
  return torch.mean(torch.sqrt(torch.mean(torch.Tensor(losses))))

train_mse, val_mse = train()

plt.plot([i for i in range(num_epochs)], train_mse, label="train")
plt.plot([i for i in range(num_epochs)], val_mse, label="val")
plt.legend()
plt.title("RMSE VS. Epoch")
plt.show()

def test():
  net.eval()
  losses = []
  for i, batch in enumerate(testloader):
    x, x_edited, y = batch
    y = y.to(device)
    x = x.to(device)
    x_edited = x_edited.to(device)
    y_hat = net(x, x_edited).squeeze()
    loss = loss_fn(y_hat, y)
    losses.append(loss)
  print(f"Average RMSE loss for Test: {torch.sqrt(torch.mean(torch.Tensor(losses)))}")
  return torch.mean(torch.sqrt(torch.mean(torch.Tensor(losses))))
test()

